{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/savinats/MAPD-B/blob/2024/Fake_News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NAIVE BAYS CLASSIFIEER FOR FAKE NEWS RECOGNITON**"
      ],
      "metadata": {
        "id": "YWP973HQ3ZJn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marco Foster & Savina Tsichli"
      ],
      "metadata": {
        "id": "nXOUjF8T3cJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Project"
      ],
      "metadata": {
        "id": "qsPHg2YO3buA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Fake news are defined by the New York Times as ”a made-up story with an intention to deceive”, with\n",
        "the intent to confuse or deceive people. They are everywhere in our daily life and they come especially\n",
        "from social media platforms and applications in the online world. Being able to distinguish fake\n",
        "contents form real news is today one of the most serious challenges facing the news industry. Naive\n",
        "Bayes classifiers are powerful algorithms that are used for text data analysis and are connected to\n",
        "classification tasks of text in multiple classes. The goal of the project is to implement a Multinomial\n",
        "Naive Bayes classifier in R and test its performances in the classification of social media posts.*"
      ],
      "metadata": {
        "id": "LHcacFAE3hKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instroduction"
      ],
      "metadata": {
        "id": "l3-uzY1X3kF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset 1: Kaggle Multiclass Fake News Dataset\n",
        "The Kaggle dataset contains 6 possible labels:\n",
        "- True (5)\n",
        "- Not-Known (4)\n",
        "- Mostly-True (3)\n",
        "- Half-True (2)\n",
        "- False (1)\n",
        "- Barely-True (0)\n",
        "\n",
        "## Dataset 2: Binary Dataset\n",
        "This dataset contains two labels:\n",
        "- Reliable (0)\n",
        "- Unreliable (1)\n",
        "\n",
        "## Preprocessing\n",
        "\n",
        "To prepare the data for classification, we employ the following steps:\n",
        "\n",
        "### Tokenization\n",
        "We split the text into individual words or tokens. Tokenization simplifies analysis by focusing on each word as a separate unit.\n",
        "\n",
        "### Stopword Removal\n",
        "Stopwords are common words like \"and\" or \"the\" that add little semantic value to the text. Removing them allows the model to focus on more important words.\n",
        "\n",
        "### Normalization\n",
        "Normalizaqtion reduces words to their base form, making words like \"running\" and \"run\" equivalent. This helps reduce the feature space by treating variations of the same word as one.\n"
      ],
      "metadata": {
        "id": "p4VOI9zV3lgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Objective"
      ],
      "metadata": {
        "id": "IFkP9GeN3olM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this project is to classify news articles into multiple categories (ranging from \"False\" to \"True\") using a **Naive Bayes classifier**. By analyzing the text in news articles, we aim to detect their factuality based on predefined labels. The dataset is split into training, validation, and test sets, and we follow standard text preprocessing techniques, including tokenization, stopword removal, and normalization.\n"
      ],
      "metadata": {
        "id": "xQjDY47T3rBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "QWbUINQq3w-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **Naive Bayes classifier** is a probabilistic machine learning model used for classification tasks. It is based on Bayes' Theorem, assuming independence between features. Despite this \"naive\" assumption, it performs well in real-world applications, especially for text classification, such as spam detection or sentiment analysis. The algorithm computes the probability of each class given a feature and selects the class with the highest likelihood. It is efficient, easy to implement, and works well with large datasets."
      ],
      "metadata": {
        "id": "lcW5QNZx30m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code"
      ],
      "metadata": {
        "id": "fnYsDqYw3y6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Packages\n",
        "package <- c(\"tokenizers\", \"tidytext\", \"dplyr\", \"tm\", \"SnowballC\", \"e1071\", \"caret\", \"readr\")\n",
        "install.packages(package)\n",
        "install.packages(\"data.table\")\n",
        "\n",
        "library(tokenizers)\n",
        "library(tidytext)\n",
        "library(dplyr)\n",
        "library(tm)\n",
        "library(SnowballC)\n",
        "library(e1071)\n",
        "library(caret)\n",
        "library(readr)\n",
        "library(data.table)"
      ],
      "metadata": {
        "id": "jw-0uJ-xw3Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eaf3d26-c2b1-47ff-9168-3c84e2482c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Training (Binary Classification)**\n",
        "\n",
        "After preparing the second dataset, a Naive Bayes model was trained using the training data.\n",
        "The data was split into training (85%) and validation (15%) sets.\n",
        "The laplace smoothing parameter was set to `1` to handle the zero probabilities of unseen words in the validation set."
      ],
      "metadata": {
        "id": "nvrKujcK38_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df <- read_csv(\"train.csv\")\n",
        "test <- read_csv(\"test.csv\")\n",
        "\n",
        "index <- nrow(df) * 0.85\n",
        "train <- df[1:index, ]\n",
        "val <- df[(index + 1):nrow(df), ]\n",
        "\n",
        "print(nrow(test))\n",
        "print(nrow(df))\n",
        "print(nrow(train))\n",
        "print(nrow(val))\n",
        "\n",
        "# Extract the Labels and Text columns\n",
        "y <- train$Labels\n",
        "Text <- train[[\"Text\"]]\n",
        "\n",
        "# Tokenize the text and store tokens in a list\n",
        "tokens_list <- lapply(Text, tokenize_words)\n",
        "#print(head(tokens_list))\n",
        "\n",
        "# Extract the Labels and Text columns\n",
        "train_y <- factor(y, levels = c(0, 1, 2, 3, 4, 5))\n",
        "val_y <- factor(val$Labels, levels = c(0, 1, 2, 3, 4, 5))\n",
        "\n",
        "TrainText <- train[[\"Text\"]]\n",
        "ValText <- val[[\"Text\"]]\n",
        "TestText <- test[[\"Text\"]]\n",
        "\n",
        "# Tokenize the text and store tokens in a list\n",
        "tokens_train <- lapply(TrainText, tokenize_words)\n",
        "tokens_train <- lapply(tokens_train, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_val <- lapply(ValText, tokenize_words)\n",
        "tokens_val <- lapply(tokens_val, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_test <- lapply(TestText, tokenize_words)\n",
        "tokens_test <- lapply(tokens_test, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "###\n",
        "#print(head(tokens_train))\n",
        "#print(head(tokens_val))\n",
        "#print(head(tokens_test))\n",
        "\n",
        "# Create a text corpus for each set\n",
        "trainCorpus <- Corpus(VectorSource(tokens_train))\n",
        "valCorpus <- Corpus(VectorSource(tokens_val))\n",
        "testCorpus <- Corpus(VectorSource(tokens_test))\n",
        "\n",
        "# Create document-term matrices\n",
        "train_dtm <- DocumentTermMatrix(trainCorpus)\n",
        "train_dtm <- removeSparseTerms(train_dtm, 0.95)\n",
        "val_dtm <- DocumentTermMatrix(valCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "test_dtm <- DocumentTermMatrix(testCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "\n",
        "# Reduce the number of features in your DTMs\n",
        "# Try removing sparse terms\n",
        "#train_dtm <- removeSparseTerms(train_dtm, 0.99) # Keep terms that appear in at least 1% of documents\n",
        "#val_dtm <- removeSparseTerms(val_dtm, 0.99)\n",
        "#test_dtm <- removeSparseTerms(test_dtm, 0.99)"
      ],
      "metadata": {
        "id": "EzJ5Lvmn4GNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Data for Modeling**"
      ],
      "metadata": {
        "id": "FaS7yDDU4IGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert DTMs to Matrices for easier manipulation\n",
        "train_matrix <- as.matrix(train_dtm)\n",
        "val_matrix <- as.matrix(val_dtm)\n",
        "test_matrix <- as.matrix(test_dtm)\n",
        "\n",
        "# Matrix columns to factors for categorization\n",
        "for (cols in colnames(train_matrix)) {\n",
        "  train_matrix[, cols] <- factor(train_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(val_matrix)) {\n",
        "  val_matrix[, cols] <- factor(val_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(test_matrix)) {\n",
        "  test_matrix[, cols] <- factor(test_matrix[, cols])\n",
        "}\n",
        "\n",
        "# Ensure Labels column is a factor with the correct levels\n",
        "train$Labels <- factor(train$Labels, levels = c(0, 1, 2, 3, 4, 5))\n",
        "val$Labels <- factor(val$Labels, levels = c(0, 1, 2, 3, 4, 5))\n",
        "\n",
        "# Combine Labels and DTM matrix in a data frame\n",
        "# The labels are combined with the training and validation matrices to prepare for model training.\n",
        "train_matrix <- data.frame(Labels = train$Labels, train_matrix)\n",
        "val_matrix <- data.frame(Labels = val$Labels, val_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tciviVqUopSx",
        "outputId": "01cb46e5-63ef-411a-a1ee-38d19bd541ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mRows: \u001b[22m\u001b[34m10240\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m3\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (2): Text, Text_Tag\n",
            "\u001b[32mdbl\u001b[39m (1): Labels\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
            "\u001b[1mRows: \u001b[22m\u001b[34m1267\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m2\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (2): Text, Text_Tag\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 1267\n",
            "[1] 10240\n",
            "[1] 8704\n",
            "[1] 1536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the Multinomial Naive Bayes classifier\n",
        "model <- naiveBayes(Labels ~ ., data = train_matrix)\n",
        "\n",
        "# Predict on validation set\n",
        "valPred <- predict(model, newdata = val_matrix)\n",
        "\n",
        "# Convert predictions and true labels to factors with the same levels\n",
        "all_levels <- c(0, 1, 2, 3, 4, 5)\n",
        "valPred <- factor(valPred, levels = all_levels)\n",
        "val_matrix$Labels <- factor(val_matrix$Labels, levels = all_levels)\n",
        "\n",
        "# Evaluate the model\n",
        "cm <- confusionMatrix(valPred, val_matrix$Labels)\n",
        "print(cm)\n",
        "\n",
        "# Predict on test set\n",
        "#testPred <- predict(model, newdata = test_final)\n",
        "\n",
        "# Ensure test predictions have the same factor levels (optional, depending on use case)\n",
        "#testPred <- factor(testPred, levels = all_levels)\n",
        "\n",
        "# Output test predictions\n",
        "#print(testPred)"
      ],
      "metadata": {
        "id": "ud3UA01cAZeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a573cdd-9c5d-4d75-8c24-e59578551b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix and Statistics\n",
            "\n",
            "          Reference\n",
            "Prediction   0   1   2   3   4   5\n",
            "         0  21  27  32  22  17  15\n",
            "         1  81 126  89 100  44  72\n",
            "         2  23  11  22  20   8  20\n",
            "         3  16  18  41  38   6  27\n",
            "         4  80  96  72  41  44  53\n",
            "         5  29  35  55  57  16  62\n",
            "\n",
            "Overall Statistics\n",
            "                                          \n",
            "               Accuracy : 0.2038          \n",
            "                 95% CI : (0.1839, 0.2248)\n",
            "    No Information Rate : 0.2038          \n",
            "    P-Value [Acc > NIR] : 0.5101          \n",
            "                                          \n",
            "                  Kappa : 0.0499          \n",
            "                                          \n",
            " Mcnemar's Test P-Value : <2e-16          \n",
            "\n",
            "Statistics by Class:\n",
            "\n",
            "                     Class: 0 Class: 1 Class: 2 Class: 3 Class: 4 Class: 5\n",
            "Sensitivity           0.08400  0.40256  0.07074  0.13669  0.32593  0.24900\n",
            "Specificity           0.91213  0.68438  0.93306  0.91415  0.75589  0.85082\n",
            "Pos Pred Value        0.15672  0.24609  0.21154  0.26027  0.11399  0.24409\n",
            "Neg Pred Value        0.83666  0.81738  0.79818  0.82734  0.92087  0.85413\n",
            "Prevalence            0.16276  0.20378  0.20247  0.18099  0.08789  0.16211\n",
            "Detection Rate        0.01367  0.08203  0.01432  0.02474  0.02865  0.04036\n",
            "Detection Prevalence  0.08724  0.33333  0.06771  0.09505  0.25130  0.16536\n",
            "Balanced Accuracy     0.49807  0.54347  0.50190  0.52542  0.54091  0.54991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final dataset**\n"
      ],
      "metadata": {
        "id": "-RTlbKbK4qzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "package <- c(\"tokenizers\", \"tidytext\", \"dplyr\", \"tm\", \"SnowballC\", \"e1071\", \"caret\", \"readr\")\n",
        "install.packages(package)\n",
        "install.packages(\"data.table\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNTcBhYfuE8z",
        "outputId": "4bd6ffd0-8f9f-4a8e-bfbb-5b09cb4fabe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘listenv’, ‘parallelly’, ‘future’, ‘globals’, ‘shape’, ‘future.apply’, ‘numDeriv’, ‘progressr’, ‘SQUAREM’, ‘diagram’, ‘lava’, ‘prodlim’, ‘iterators’, ‘clock’, ‘gower’, ‘hardhat’, ‘ipred’, ‘timeDate’, ‘janeaustenr’, ‘NLP’, ‘slam’, ‘BH’, ‘proxy’, ‘foreach’, ‘ModelMetrics’, ‘plyr’, ‘pROC’, ‘recipes’, ‘reshape2’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "library(tokenizers)\n",
        "library(tidytext)\n",
        "library(dplyr)\n",
        "library(tm)\n",
        "library(SnowballC)\n",
        "library(e1071)\n",
        "library(caret)\n",
        "library(readr)\n",
        "library(data.table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txmowkhKEFsn",
        "outputId": "e859cf3a-26bb-4615-8d99-8e6cd4e20f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Attaching package: ‘dplyr’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:stats’:\n",
            "\n",
            "    filter, lag\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:base’:\n",
            "\n",
            "    intersect, setdiff, setequal, union\n",
            "\n",
            "\n",
            "Loading required package: NLP\n",
            "\n",
            "Loading required package: ggplot2\n",
            "\n",
            "\n",
            "Attaching package: ‘ggplot2’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:NLP’:\n",
            "\n",
            "    annotate\n",
            "\n",
            "\n",
            "Loading required package: lattice\n",
            "\n",
            "\n",
            "Attaching package: ‘data.table’\n",
            "\n",
            "\n",
            "The following objects are masked from ‘package:dplyr’:\n",
            "\n",
            "    between, first, last\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same work on new dataset\n",
        "\n",
        "df <- read_csv(\"train2.csv\")\n",
        "test <- read_csv(\"test2.csv\")\n",
        "\n",
        "index <- nrow(df) * 0.85\n",
        "train <- df[1:index, ]\n",
        "val <- df[(index + 1):nrow(df), ]\n",
        "\n",
        "print(nrow(test))\n",
        "print(nrow(df))\n",
        "print(nrow(train))\n",
        "print(nrow(val))\n",
        "\n",
        "# Extract the Labels and Text columns\n",
        "y <- train$label\n",
        "Text <- train[[\"text\"]]\n",
        "\n",
        "# Tokenize the text and store tokens in a list\n",
        "tokens_list <- lapply(Text, tokenize_words)\n",
        "#print(head(tokens_list))\n",
        "\n",
        "# Extract the Labels and Text columns\n",
        "train_y <- factor(y, levels = c(0, 1))\n",
        "val_y <- factor(val$label, levels = c(0, 1))\n",
        "\n",
        "TrainText <- train[[\"text\"]]\n",
        "ValText <- val[[\"text\"]]\n",
        "TestText <- test[[\"text\"]]\n",
        "\n",
        "# Tokenize the text and store tokens in a list\n",
        "tokens_train <- lapply(TrainText, tokenize_words)\n",
        "tokens_train <- lapply(tokens_train, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_val <- lapply(ValText, tokenize_words)\n",
        "tokens_val <- lapply(tokens_val, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "tokens_test <- lapply(TestText, tokenize_words)\n",
        "tokens_test <- lapply(tokens_test, function(x) setdiff(x, stopwords(\"en\")))\n",
        "\n",
        "###\n",
        "#print(head(tokens_train))\n",
        "#print(head(tokens_val))\n",
        "#print(head(tokens_test))\n",
        "\n",
        "# Create a text corpus for each set\n",
        "trainCorpus <- Corpus(VectorSource(tokens_train))\n",
        "valCorpus <- Corpus(VectorSource(tokens_val))\n",
        "testCorpus <- Corpus(VectorSource(tokens_test))\n",
        "\n",
        "# Create document-term matrices\n",
        "train_dtm <- DocumentTermMatrix(trainCorpus)\n",
        "train_dtm <- removeSparseTerms(train_dtm, 0.95)\n",
        "val_dtm <- DocumentTermMatrix(valCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "test_dtm <- DocumentTermMatrix(testCorpus, control = list(dictionary = Terms(train_dtm)))\n",
        "\n",
        "# Reduce the number of features in your DTMs\n",
        "#train_dtm <- removeSparseTerms(train_dtm, 0.99) # Keep terms that appear in at least 1% of documents\n",
        "#val_dtm <- removeSparseTerms(val_dtm, 0.99)\n",
        "#test_dtm <- removeSparseTerms(test_dtm, 0.99)\n",
        "\n",
        "train_matrix <- as.matrix(train_dtm)\n",
        "val_matrix <- as.matrix(val_dtm)\n",
        "test_matrix <- as.matrix(test_dtm)\n",
        "\n",
        "for (cols in colnames(train_matrix)) {\n",
        "  train_matrix[, cols] <- factor(train_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(val_matrix)) {\n",
        "  val_matrix[, cols] <- factor(val_matrix[, cols])\n",
        "}\n",
        "\n",
        "for (cols in colnames(test_matrix)) {\n",
        "  test_matrix[, cols] <- factor(test_matrix[, cols])\n",
        "}\n",
        "\n",
        "train_matrix <- data.frame(Labels = as.factor(train$label), train_matrix)\n",
        "val_matrix <- data.frame(Labels = as.factor(val$label), val_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8_ZfMVrrGBv",
        "outputId": "6fa84f30-8444-4c11-9220-6d66d2f72d1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mRows: \u001b[22m\u001b[34m20800\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m5\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (3): title, author, text\n",
            "\u001b[32mdbl\u001b[39m (2): id, label\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n",
            "\u001b[1mRows: \u001b[22m\u001b[34m5200\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m4\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (3): title, author, text\n",
            "\u001b[32mdbl\u001b[39m (1): id\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] 5200\n",
            "[1] 20800\n",
            "[1] 17680\n",
            "[1] 3120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all(colnames(train_matrix) == colnames(val_matrix))  # Should return TRUE\n",
        "all(colnames(train_matrix) == colnames(test_matrix))  # Should return TRUE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "XH_eZhnAMU-N",
        "outputId": "db060729-d336-4d5f-e3f6-c45754e1fc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "TRUE"
            ],
            "text/markdown": "TRUE",
            "text/latex": "TRUE",
            "text/plain": [
              "[1] TRUE"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in colnames(train_matrix) == colnames(test_matrix):\n",
            "“longer object length is not a multiple of shorter object length”\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "FALSE"
            ],
            "text/markdown": "FALSE",
            "text/latex": "FALSE",
            "text/plain": [
              "[1] FALSE"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_matrix$Labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "U7lxNcs5pDH-",
        "outputId": "57cc6174-b030-4f15-cf85-66c9ac0d48c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              ".list-inline {list-style: none; margin:0; padding: 0}\n",
              ".list-inline>li {display: inline-block}\n",
              ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "</style>\n",
              "<ol class=list-inline><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>⋯</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>1</li><li>0</li><li>0</li><li>0</li><li>1</li><li>1</li><li>0</li><li>1</li><li>0</li><li>1</li><li>1</li><li>1</li></ol>\n",
              "\n",
              "<details>\n",
              "\t<summary style=display:list-item;cursor:pointer>\n",
              "\t\t<strong>Levels</strong>:\n",
              "\t</summary>\n",
              "\t<style>\n",
              "\t.list-inline {list-style: none; margin:0; padding: 0}\n",
              "\t.list-inline>li {display: inline-block}\n",
              "\t.list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
              "\t</style>\n",
              "\t<ol class=list-inline><li>'0'</li><li>'1'</li></ol>\n",
              "</details>"
            ],
            "text/markdown": "1. 1\n2. 0\n3. 1\n4. 1\n5. 1\n6. 0\n7. 1\n8. 0\n9. 0\n10. 0\n11. 0\n12. 0\n13. 1\n14. 1\n15. 1\n16. 0\n17. 0\n18. 1\n19. 1\n20. 0\n21. 1\n22. 0\n23. 0\n24. 1\n25. 0\n26. 1\n27. 0\n28. 1\n29. 0\n30. 0\n31. 0\n32. 1\n33. 0\n34. 0\n35. 0\n36. 0\n37. 1\n38. 1\n39. 0\n40. 0\n41. 0\n42. 1\n43. 0\n44. 1\n45. 0\n46. 0\n47. 1\n48. 1\n49. 0\n50. 0\n51. 1\n52. 1\n53. 0\n54. 1\n55. 1\n56. 1\n57. 1\n58. 1\n59. 1\n60. 0\n61. 0\n62. 1\n63. 0\n64. 1\n65. 0\n66. 0\n67. 1\n68. 1\n69. 0\n70. 1\n71. 1\n72. 1\n73. 0\n74. 0\n75. 0\n76. 0\n77. 1\n78. 1\n79. 0\n80. 0\n81. 0\n82. 0\n83. 1\n84. 1\n85. 0\n86. 0\n87. 1\n88. 0\n89. 1\n90. 0\n91. 0\n92. 1\n93. 0\n94. 1\n95. 0\n96. 0\n97. 1\n98. 0\n99. 0\n100. 0\n101. 1\n102. 0\n103. 0\n104. 1\n105. 0\n106. 1\n107. 0\n108. 1\n109. 0\n110. 0\n111. 0\n112. 0\n113. 1\n114. 0\n115. 1\n116. 0\n117. 0\n118. 0\n119. 0\n120. 0\n121. 1\n122. 1\n123. 1\n124. 0\n125. 1\n126. 1\n127. 1\n128. 0\n129. 0\n130. 0\n131. 0\n132. 1\n133. 0\n134. 0\n135. 1\n136. 1\n137. 0\n138. 1\n139. 1\n140. 0\n141. 1\n142. 1\n143. 1\n144. 1\n145. 1\n146. 1\n147. 1\n148. 0\n149. 1\n150. 0\n151. 1\n152. 1\n153. 0\n154. 0\n155. 0\n156. 0\n157. 1\n158. 0\n159. 0\n160. 1\n161. 0\n162. 1\n163. 1\n164. 1\n165. 1\n166. 0\n167. 0\n168. 0\n169. 0\n170. 1\n171. 1\n172. 0\n173. 1\n174. 1\n175. 0\n176. 1\n177. 0\n178. 1\n179. 1\n180. 1\n181. 0\n182. 0\n183. 0\n184. 0\n185. 1\n186. 0\n187. 1\n188. 0\n189. 0\n190. 1\n191. 1\n192. 0\n193. 1\n194. 1\n195. 1\n196. 1\n197. 1\n198. 1\n199. 1\n200. 0\n201. ⋯\n202. 0\n203. 0\n204. 1\n205. 0\n206. 1\n207. 1\n208. 0\n209. 0\n210. 1\n211. 0\n212. 1\n213. 1\n214. 0\n215. 1\n216. 0\n217. 1\n218. 0\n219. 1\n220. 1\n221. 1\n222. 0\n223. 1\n224. 1\n225. 1\n226. 1\n227. 1\n228. 0\n229. 1\n230. 0\n231. 0\n232. 1\n233. 0\n234. 0\n235. 0\n236. 0\n237. 1\n238. 0\n239. 1\n240. 0\n241. 1\n242. 0\n243. 0\n244. 0\n245. 0\n246. 1\n247. 1\n248. 1\n249. 0\n250. 0\n251. 0\n252. 1\n253. 1\n254. 0\n255. 0\n256. 0\n257. 1\n258. 1\n259. 0\n260. 1\n261. 0\n262. 0\n263. 1\n264. 1\n265. 0\n266. 1\n267. 0\n268. 1\n269. 1\n270. 1\n271. 1\n272. 0\n273. 1\n274. 1\n275. 1\n276. 1\n277. 0\n278. 0\n279. 1\n280. 0\n281. 1\n282. 1\n283. 0\n284. 0\n285. 0\n286. 0\n287. 0\n288. 1\n289. 1\n290. 0\n291. 1\n292. 1\n293. 1\n294. 0\n295. 0\n296. 0\n297. 1\n298. 1\n299. 1\n300. 0\n301. 1\n302. 0\n303. 1\n304. 1\n305. 0\n306. 0\n307. 1\n308. 0\n309. 0\n310. 1\n311. 0\n312. 1\n313. 0\n314. 0\n315. 1\n316. 1\n317. 1\n318. 1\n319. 1\n320. 1\n321. 1\n322. 0\n323. 0\n324. 0\n325. 1\n326. 0\n327. 0\n328. 1\n329. 1\n330. 0\n331. 1\n332. 1\n333. 0\n334. 0\n335. 1\n336. 0\n337. 1\n338. 0\n339. 1\n340. 1\n341. 0\n342. 1\n343. 1\n344. 1\n345. 0\n346. 1\n347. 0\n348. 1\n349. 0\n350. 1\n351. 1\n352. 0\n353. 0\n354. 1\n355. 1\n356. 1\n357. 1\n358. 1\n359. 0\n360. 1\n361. 1\n362. 0\n363. 1\n364. 0\n365. 0\n366. 0\n367. 1\n368. 0\n369. 0\n370. 1\n371. 1\n372. 0\n373. 1\n374. 1\n375. 1\n376. 0\n377. 0\n378. 0\n379. 1\n380. 1\n381. 0\n382. 0\n383. 1\n384. 1\n385. 0\n386. 0\n387. 0\n388. 1\n389. 1\n390. 1\n391. 0\n392. 0\n393. 0\n394. 1\n395. 1\n396. 0\n397. 1\n398. 0\n399. 1\n400. 1\n401. 1\n\n\n\n**Levels**: 1. '0'\n2. '1'\n\n\n",
            "text/latex": "\\begin{enumerate*}\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item ⋯\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\item 0\n\\item 0\n\\item 0\n\\item 1\n\\item 1\n\\item 0\n\\item 1\n\\item 0\n\\item 1\n\\item 1\n\\item 1\n\\end{enumerate*}\n\n\\emph{Levels}: \\begin{enumerate*}\n\\item '0'\n\\item '1'\n\\end{enumerate*}\n",
            "text/plain": [
              "    [1] 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0\n",
              "   [37] 1 1 0 0 0 1 0 1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 1 0 1 1 1\n",
              "   [73] 0 0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1\n",
              "  [109] 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1\n",
              "  [145] 1 1 1 0 1 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 1 1 0 1 0 1 1 1\n",
              "  [181] 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1 1\n",
              "  [217] 1 0 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0 1 0\n",
              "  [253] 0 1 1 0 0 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 0\n",
              "  [289] 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 0\n",
              "  [325] 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 0 1 0 0 1 0 0 0\n",
              "  [361] 1 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 1 1 0 0\n",
              "  [397] 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 1 0\n",
              "  [433] 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 1 1 1\n",
              "  [469] 0 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0\n",
              "  [505] 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1\n",
              "  [541] 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 1 1 0 1\n",
              "  [577] 0 1 1 1 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1\n",
              "  [613] 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 1\n",
              "  [649] 0 0 1 1 1 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 0 1 0 1 1 1\n",
              "  [685] 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1\n",
              "  [721] 0 1 0 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 1 0 1 1 1 0 1 0 0 0 0 0 1 1 1\n",
              "  [757] 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1\n",
              "  [793] 0 1 1 0 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 1 0 1 1 1 0 0 0 0 0 1 1 1\n",
              "  [829] 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 1 1 1 1 0 0\n",
              "  [865] 0 0 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 0 1 0\n",
              "  [901] 1 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
              "  [937] 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1\n",
              "  [973] 0 0 1 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0\n",
              " [1009] 0 1 0 0 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0\n",
              " [1045] 0 1 1 0 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 1\n",
              " [1081] 0 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 0 0\n",
              " [1117] 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1\n",
              " [1153] 1 0 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1\n",
              " [1189] 1 0 0 1 0 0 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 1 0\n",
              " [1225] 0 0 1 0 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 0 0 1\n",
              " [1261] 1 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 1 0 0\n",
              " [1297] 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 1 1 0 0 0 1 1 1\n",
              " [1333] 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1\n",
              " [1369] 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0\n",
              " [1405] 1 1 0 0 0 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 0 1\n",
              " [1441] 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
              " [1477] 0 0 1 1 0 1 1 1 0 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 0 1 1 1 0 1\n",
              " [1513] 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1\n",
              " [1549] 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 1 0 1 1 0 1\n",
              " [1585] 1 1 1 1 0 0 1 1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1\n",
              " [1621] 0 1 0 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 0 0 1 1\n",
              " [1657] 1 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 1 0 1 0 1 0 1\n",
              " [1693] 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0\n",
              " [1729] 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1\n",
              " [1765] 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 0\n",
              " [1801] 1 0 1 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 0 1\n",
              " [1837] 1 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 1\n",
              " [1873] 0 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1\n",
              " [1909] 1 1 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 1 1\n",
              " [1945] 1 0 1 0 0 0 0 1 1 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0\n",
              " [1981] 0 1 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 1 1\n",
              " [2017] 1 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 1 0 0\n",
              " [2053] 1 0 0 1 1 0 1 0 0 1 0 0 0 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 0\n",
              " [2089] 1 1 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 1 0 1 1 1 1 1 1 0\n",
              " [2125] 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 1 0\n",
              " [2161] 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0\n",
              " [2197] 0 0 1 1 0 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0\n",
              " [2233] 0 1 0 1 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
              " [2269] 0 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1\n",
              " [2305] 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1\n",
              " [2341] 1 0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 0 0 1 1 0 0 1\n",
              " [2377] 0 0 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 1 1 0 1 0 1 0 1\n",
              " [2413] 0 0 1 0 1 1 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0\n",
              " [2449] 1 1 0 1 0 1 0 1 0 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0\n",
              " [2485] 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 1\n",
              " [2521] 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 1 0 0\n",
              " [2557] 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0\n",
              " [2593] 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
              " [2629] 1 0 1 0 0 1 0 1 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0\n",
              " [2665] 0 1 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 0\n",
              " [2701] 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 1 1\n",
              " [2737] 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
              " [2773] 1 1 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 1\n",
              " [2809] 0 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0\n",
              " [2845] 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 1\n",
              " [2881] 1 1 0 1 0 0 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 1 1 1\n",
              " [2917] 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0\n",
              " [2953] 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1\n",
              " [2989] 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 1 1\n",
              " [3025] 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0\n",
              " [3061] 0 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 1 0 1 0 0\n",
              " [3097] 1 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1\n",
              " [3133] 1 1 0 0 0 1 0 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 1 0 0 1 1\n",
              " [3169] 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 1 0 0\n",
              " [3205] 1 1 1 1 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 1\n",
              " [3241] 1 0 1 0 1 0 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1 1 1\n",
              " [3277] 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 0 1 1 0 1 1 1 1 0\n",
              " [3313] 1 1 1 1 0 0 0 1 1 1 1 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1\n",
              " [3349] 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1\n",
              " [3385] 0 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1\n",
              " [3421] 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 1 0\n",
              " [3457] 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n",
              " [3493] 0 0 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 0 1 0 1 0 0 1 1 1\n",
              " [3529] 0 0 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1\n",
              " [3565] 0 1 1 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
              " [3601] 1 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 1 0 1 0 0 0 1 1 1 1 0 1 1 1 0\n",
              " [3637] 1 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 1 1\n",
              " [3673] 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 0 1 0 1\n",
              " [3709] 1 1 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 0 1 1 1 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1\n",
              " [3745] 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0\n",
              " [3781] 1 0 0 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0\n",
              " [3817] 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 0 0\n",
              " [3853] 1 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 0\n",
              " [3889] 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 1 0 1 1\n",
              " [3925] 1 1 1 1 1 1 1 0 1 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0\n",
              " [3961] 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 1\n",
              " [3997] 1 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 1 0\n",
              " [4033] 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1\n",
              " [4069] 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 0 0\n",
              " [4105] 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1\n",
              " [4141] 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
              " [4177] 0 0 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 0 1\n",
              " [4213] 0 1 0 0 0 1 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 1 1\n",
              " [4249] 1 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0\n",
              " [4285] 0 0 0 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 0 0\n",
              " [4321] 1 0 0 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 1\n",
              " [4357] 1 0 1 0 1 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0\n",
              " [4393] 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 0 0 1 1 0 0\n",
              " [4429] 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
              " [4465] 1 1 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 1 0 1 0\n",
              " [4501] 1 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 1 0\n",
              " [4537] 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0\n",
              " [4573] 0 0 1 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 1 1 1 0 0 1 0 1 1 1 0 0 0\n",
              " [4609] 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 0 0 0 1 1 0 0 0 1 1\n",
              " [4645] 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 1 1 1\n",
              " [4681] 0 0 1 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0\n",
              " [4717] 0 0 1 1 1 0 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1\n",
              " [4753] 0 1 1 1 0 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0 0\n",
              " [4789] 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0 1 0\n",
              " [4825] 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 1 0 0 0\n",
              " [4861] 0 0 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0\n",
              " [4897] 1 0 1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0\n",
              " [4933] 0 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0\n",
              " [4969] 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0\n",
              " [5005] 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 0\n",
              " [5041] 1 1 0 0 1 0 0 1 1 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1\n",
              " [5077] 1 0 1 1 1 0 0 1 0 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 1 0 0 0\n",
              " [5113] 1 1 1 0 1 1 0 0 0 1 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 0\n",
              " [5149] 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1\n",
              " [5185] 1 1 1 1 0 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 1 1 0\n",
              " [5221] 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0\n",
              " [5257] 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 1 0\n",
              " [5293] 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 0 0\n",
              " [5329] 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 1\n",
              " [5365] 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0\n",
              " [5401] 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 0 0 0 0\n",
              " [5437] 0 0 1 0 0 1 0 0 1 1 1 1 1 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0\n",
              " [5473] 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
              " [5509] 1 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
              " [5545] 0 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 0\n",
              " [5581] 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
              " [5617] 1 0 1 1 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0\n",
              " [5653] 0 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1\n",
              " [5689] 1 1 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0\n",
              " [5725] 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 1\n",
              " [5761] 0 0 0 0 1 0 1 0 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1\n",
              " [5797] 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0\n",
              " [5833] 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
              " [5869] 1 1 1 0 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 1 1 0\n",
              " [5905] 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 1 0 0 1 0 0 0 1\n",
              " [5941] 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 1 1\n",
              " [5977] 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 1\n",
              " [6013] 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0\n",
              " [6049] 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
              " [6085] 1 1 0 0 0 1 0 1 0 0 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 1\n",
              " [6121] 1 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 1 0 0\n",
              " [6157] 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 1 0\n",
              " [6193] 0 1 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1\n",
              " [6229] 1 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 1 1 0 1 0 0\n",
              " [6265] 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 0 1\n",
              " [6301] 0 1 1 1 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0\n",
              " [6337] 1 1 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 0\n",
              " [6373] 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 0 0 0\n",
              " [6409] 1 0 0 1 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
              " [6445] 0 0 1 1 1 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 0\n",
              " [6481] 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
              " [6517] 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1\n",
              " [6553] 0 1 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0\n",
              " [6589] 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1 0 0 1 1 1 0\n",
              " [6625] 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1 0 1\n",
              " [6661] 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0\n",
              " [6697] 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
              " [6733] 0 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0\n",
              " [6769] 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 1\n",
              " [6805] 1 0 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 1 1\n",
              " [6841] 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1\n",
              " [6877] 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1\n",
              " [6913] 0 0 1 1 1 0 0 0 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 0 0 1 0 1 1 0 1\n",
              " [6949] 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 1 1 1 0 0 1 0 1 1\n",
              " [6985] 1 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0\n",
              " [7021] 1 1 1 1 0 1 0 1 0 0 0 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1\n",
              " [7057] 1 1 1 0 0 1 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1\n",
              " [7093] 1 1 1 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 0 0 1 0 1 0 1 1\n",
              " [7129] 1 0 1 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1\n",
              " [7165] 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1 1\n",
              " [7201] 0 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 1 0 0 1 1 0 0 1 0\n",
              " [7237] 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0\n",
              " [7273] 1 0 0 1 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0\n",
              " [7309] 1 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1\n",
              " [7345] 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 0\n",
              " [7381] 0 1 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0\n",
              " [7417] 0 0 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1\n",
              " [7453] 0 0 0 0 1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0\n",
              " [7489] 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 0 1 0\n",
              " [7525] 1 1 0 0 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0\n",
              " [7561] 1 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1\n",
              " [7597] 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0\n",
              " [7633] 0 1 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 1 0 1 1 0\n",
              " [7669] 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 1 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 0\n",
              " [7705] 1 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0\n",
              " [7741] 0 1 0 0 0 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 0 1 0 1\n",
              " [7777] 1 0 0 1 1 1 0 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 0 0 1 1 1 0 1\n",
              " [7813] 1 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 1\n",
              " [7849] 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1\n",
              " [7885] 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0\n",
              " [7921] 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 1\n",
              " [7957] 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0\n",
              " [7993] 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
              " [8029] 0 1 1 1 0 0 1 1 1 1 0 0 1 0 0 0 1 1 1 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 0 0\n",
              " [8065] 1 0 0 0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0\n",
              " [8101] 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 0\n",
              " [8137] 0 1 1 0 1 0 0 0 0 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0\n",
              " [8173] 0 1 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1\n",
              " [8209] 1 0 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 1 1 0 0 1\n",
              " [8245] 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0\n",
              " [8281] 0 1 1 1 1 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 1 0 0 1 1\n",
              " [8317] 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1\n",
              " [8353] 1 0 1 0 1 1 1 0 0 1 0 1 1 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 1 1 0\n",
              " [8389] 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 1 0 0 1 0 0 0 0 0 1 1 0 0 1\n",
              " [8425] 1 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1\n",
              " [8461] 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1\n",
              " [8497] 0 1 1 0 0 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0\n",
              " [8533] 0 1 1 1 0 1 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0\n",
              " [8569] 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
              " [8605] 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0\n",
              " [8641] 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1\n",
              " [8677] 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 1 1\n",
              " [8713] 1 0 1 1 1 1 0 1 0 0 1 0 0 1 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 1\n",
              " [8749] 0 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0\n",
              " [8785] 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1 0 1 1 0 1\n",
              " [8821] 1 0 0 0 0 0 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 0 1 0 0 0 0\n",
              " [8857] 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 1 1 1 0 1 0 0 0 1 0\n",
              " [8893] 1 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0\n",
              " [8929] 0 0 1 0 0 0 1 1 0 1 1 1 0 1 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 1 1 1 0 0 1 0\n",
              " [8965] 1 0 1 0 0 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1 1 1 0\n",
              " [9001] 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1\n",
              " [9037] 0 1 1 0 1 1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0\n",
              " [9073] 0 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1\n",
              " [9109] 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0\n",
              " [9145] 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0\n",
              " [9181] 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 1\n",
              " [9217] 1 1 0 1 0 1 1 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1\n",
              " [9253] 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0\n",
              " [9289] 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 0 0 0\n",
              " [9325] 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1\n",
              " [9361] 0 1 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 1\n",
              " [9397] 1 0 0 1 0 0 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1\n",
              " [9433] 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1\n",
              " [9469] 1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 0\n",
              " [9505] 1 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 1 0\n",
              " [9541] 0 1 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1\n",
              " [9577] 0 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0 1 0\n",
              " [9613] 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0\n",
              " [9649] 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 0 1 0 1 1 0 1 1\n",
              " [9685] 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 1 0\n",
              " [9721] 0 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1\n",
              " [9757] 0 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 1\n",
              " [9793] 0 1 0 1 1 1 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 0 1\n",
              " [9829] 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 1 1\n",
              " [9865] 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
              " [9901] 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 0 0 1 0 0 0 0 1\n",
              " [9937] 0 0 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0\n",
              " [9973] 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 0\n",
              "[10009] 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 1\n",
              "[10045] 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0\n",
              "[10081] 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 1\n",
              "[10117] 0 0 1 1 1 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1\n",
              "[10153] 1 1 0 1 1 0 1 0 0 1 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1 0 0 0 0 1 1\n",
              "[10189] 0 1 1 0 0 0 1 1 0 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 0\n",
              "[10225] 0 0 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0\n",
              "[10261] 0 1 0 1 1 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 1\n",
              "[10297] 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 1 0 1\n",
              "[10333] 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 0\n",
              "[10369] 1 0 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1\n",
              "[10405] 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 0\n",
              "[10441] 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0\n",
              "[10477] 0 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0\n",
              "[10513] 1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0 1\n",
              "[10549] 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0\n",
              "[10585] 0 1 0 0 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1\n",
              "[10621] 1 1 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 0\n",
              "[10657] 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 1\n",
              "[10693] 1 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 1\n",
              "[10729] 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0\n",
              "[10765] 1 1 0 1 0 0 0 1 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 0\n",
              "[10801] 1 1 1 0 1 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 1 0 1 0 1 0 0 0 1\n",
              "[10837] 0 1 1 0 1 1 0 1 0 0 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 0 1 0 1 0 0\n",
              "[10873] 1 1 1 0 1 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1\n",
              "[10909] 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 0 1 1 1\n",
              "[10945] 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1\n",
              "[10981] 0 0 0 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 0 1 0\n",
              "[11017] 1 1 1 0 1 0 1 0 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0\n",
              "[11053] 1 0 0 1 0 1 1 0 0 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0\n",
              "[11089] 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1 1 0\n",
              "[11125] 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0\n",
              "[11161] 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0\n",
              "[11197] 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 1 1 1\n",
              "[11233] 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 1 1 1 1 0 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1\n",
              "[11269] 0 1 1 0 0 0 0 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 1 1\n",
              "[11305] 0 1 0 0 1 0 0 0 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0\n",
              "[11341] 0 0 0 1 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
              "[11377] 1 0 1 1 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1\n",
              "[11413] 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 1 0 0 0 1 0\n",
              "[11449] 0 0 1 1 0 1 1 0 1 1 0 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 1\n",
              "[11485] 1 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
              "[11521] 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 1 1 1 1\n",
              "[11557] 0 1 0 0 1 1 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 1 0 1 0 1 0 1 1\n",
              "[11593] 1 0 0 1 1 1 0 0 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 0 0 0\n",
              "[11629] 0 0 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
              "[11665] 1 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 0\n",
              "[11701] 1 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 1 1 0\n",
              "[11737] 1 0 0 0 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1\n",
              "[11773] 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0\n",
              "[11809] 1 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 0\n",
              "[11845] 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0\n",
              "[11881] 0 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
              "[11917] 1 0 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 1 0 0 1\n",
              "[11953] 0 1 1 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 0 0\n",
              "[11989] 1 1 0 0 0 0 1 1 0 0 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1\n",
              "[12025] 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1\n",
              "[12061] 0 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 0 1\n",
              "[12097] 1 1 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0\n",
              "[12133] 0 1 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 0\n",
              "[12169] 1 1 1 0 0 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 0 1 0 1\n",
              "[12205] 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 1\n",
              "[12241] 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1\n",
              "[12277] 0 1 1 1 0 0 1 1 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
              "[12313] 1 0 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1 1 1\n",
              "[12349] 1 0 1 0 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 0 0 1 1 1 1 1\n",
              "[12385] 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 1 0\n",
              "[12421] 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 1 1 0 1 0 0\n",
              "[12457] 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1\n",
              "[12493] 1 0 1 1 1 1 0 1 0 0 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 0 0 0 1\n",
              "[12529] 1 0 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 1\n",
              "[12565] 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 0 0 0 0\n",
              "[12601] 0 0 1 1 1 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 0 1\n",
              "[12637] 0 1 0 0 1 1 0 0 0 1 0 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0\n",
              "[12673] 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 1 0 1 1 1\n",
              "[12709] 0 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 0 1 0\n",
              "[12745] 1 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1\n",
              "[12781] 1 0 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 1 1 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 0 1\n",
              "[12817] 1 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 1 0 1 0 0 1 0 0 0\n",
              "[12853] 1 0 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0 1\n",
              "[12889] 1 0 1 1 1 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 1 0\n",
              "[12925] 1 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0\n",
              "[12961] 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 0\n",
              "[12997] 1 1 0 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 1 1 1 1 0 0\n",
              "[13033] 0 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 1 1 1 0 1 0 0 0 1 1 1 0 0 0 0 1 1\n",
              "[13069] 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 0\n",
              "[13105] 1 1 0 1 1 1 0 0 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 1 0\n",
              "[13141] 0 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0\n",
              "[13177] 0 1 0 1 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0\n",
              "[13213] 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 1 1\n",
              "[13249] 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
              "[13285] 0 1 0 1 0 1 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0 0 0 0 0 1 0 0 1 0\n",
              "[13321] 1 1 1 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0\n",
              "[13357] 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
              "[13393] 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 1 0 0 0 0\n",
              "[13429] 1 0 1 1 0 1 1 0 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 1 1 1 1 0 1 1\n",
              "[13465] 1 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 1 0\n",
              "[13501] 0 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1\n",
              "[13537] 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1\n",
              "[13573] 1 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 0 0\n",
              "[13609] 0 1 1 0 0 1 0 0 0 0 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1\n",
              "[13645] 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0 0 0 1\n",
              "[13681] 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 1 0 1 1 1\n",
              "[13717] 1 0 1 1 1 0 1 0 0 1 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1\n",
              "[13753] 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1\n",
              "[13789] 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 1 0 1 1 1\n",
              "[13825] 1 0 1 1 0 0 1 1 1 0 1 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1\n",
              "[13861] 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 1 0 0\n",
              "[13897] 0 0 1 1 0 1 1 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0\n",
              "[13933] 1 1 1 1 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1\n",
              "[13969] 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0\n",
              "[14005] 0 1 1 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 1\n",
              "[14041] 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1 1 1 0\n",
              "[14077] 1 1 0 0 0 1 1 0 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 0\n",
              "[14113] 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 0 0 0 1 1 1\n",
              "[14149] 0 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 0 1\n",
              "[14185] 1 1 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 0 1 0 1 1 0 0 1 1 1 1 1 0 1 0 1 1 1 1\n",
              "[14221] 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 0 1 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1\n",
              "[14257] 1 1 1 0 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 0 0 0\n",
              "[14293] 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 1 1 1 0 1 1 0 1 0 0 1 1 0 0 1\n",
              "[14329] 0 1 1 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1\n",
              "[14365] 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 1\n",
              "[14401] 1 1 0 1 0 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 1 1 1 1\n",
              "[14437] 1 1 0 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 1 1 0 0 1\n",
              "[14473] 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 1 1 1 0 0\n",
              "[14509] 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 1 0\n",
              "[14545] 1 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0\n",
              "[14581] 0 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 0 1 1 0 1 0 1 1 0 1 0\n",
              "[14617] 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 0 0 1 1 1 0 1 1 1\n",
              "[14653] 1 0 1 0 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0\n",
              "[14689] 1 1 0 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0\n",
              "[14725] 0 1 0 0 1 1 0 0 1 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 0 1 1 0\n",
              "[14761] 0 1 0 1 0 0 1 1 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 0 1 1 1 0 1 1 0 1 0 0 1\n",
              "[14797] 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1\n",
              "[14833] 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0\n",
              "[14869] 0 0 1 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0\n",
              "[14905] 0 1 0 1 1 0 1 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0\n",
              "[14941] 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1 0 1 1 1 0\n",
              "[14977] 0 1 0 0 1 1 1 0 1 0 0 1 1 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 0\n",
              "[15013] 0 1 0 1 0 0 1 0 0 1 1 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 1 1 1 1\n",
              "[15049] 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 1\n",
              "[15085] 0 1 0 1 0 1 0 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 0\n",
              "[15121] 0 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 1 1 0 1 1 0 0\n",
              "[15157] 1 1 0 0 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1\n",
              "[15193] 0 0 1 1 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
              "[15229] 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 0 1 1 1\n",
              "[15265] 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 0 1 0 0 1\n",
              "[15301] 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 0\n",
              "[15337] 0 0 0 1 0 0 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 1\n",
              "[15373] 0 0 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 1 0\n",
              "[15409] 0 1 0 1 1 0 1 0 1 0 0 1 0 0 1 1 0 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 0 0\n",
              "[15445] 1 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0\n",
              "[15481] 0 0 0 1 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 1 1\n",
              "[15517] 0 0 0 1 1 0 0 1 1 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 1\n",
              "[15553] 0 0 1 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1\n",
              "[15589] 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0\n",
              "[15625] 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 1 1 0\n",
              "[15661] 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0\n",
              "[15697] 1 0 1 0 1 1 1 1 1 1 1 0 1 1 0 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 0 0 1 0 1 0\n",
              "[15733] 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1\n",
              "[15769] 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 0 0\n",
              "[15805] 1 1 1 1 0 0 0 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 0 1 1 1 0\n",
              "[15841] 1 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0\n",
              "[15877] 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1\n",
              "[15913] 1 1 0 1 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0\n",
              "[15949] 0 0 1 0 1 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 1\n",
              "[15985] 0 0 0 1 1 0 0 1 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 1 1 1 0 0 0 0 0\n",
              "[16021] 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1 0 1 0 0 1 1 0 0 1 1 0 0\n",
              "[16057] 1 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 0 0\n",
              "[16093] 0 1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 1 0\n",
              "[16129] 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0 1 0 1 1 1 0 0 0 1 0 1 0 0 0 1\n",
              "[16165] 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 0 0 1 0 0 0 0 1\n",
              "[16201] 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 0 0 1\n",
              "[16237] 1 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 0 0 1 0 0\n",
              "[16273] 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 1 1 1 0 1 0 1 0 1 0 0 1 1 1 1\n",
              "[16309] 0 0 1 0 0 0 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n",
              "[16345] 1 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 0 0 0 1 0 0 1 0 1\n",
              "[16381] 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 1 0 1\n",
              "[16417] 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0\n",
              "[16453] 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 1\n",
              "[16489] 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 0 1 1 1 0 1\n",
              "[16525] 1 0 1 1 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 1 0 1 1 1 0\n",
              "[16561] 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0\n",
              "[16597] 1 1 1 0 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 1 1 0 1 0 1 1\n",
              "[16633] 0 1 0 1 1 0 1 0 1 0 1 0 0 1 0 1 0 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1\n",
              "[16669] 1 0 0 1 0 1 0 1 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1\n",
              "[16705] 0 0 0 1 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 0 0 1 1 1 0\n",
              "[16741] 0 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 1 1 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0\n",
              "[16777] 0 0 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 0 1 0 1 1 1 0\n",
              "[16813] 1 1 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0 1 1 0\n",
              "[16849] 0 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 1\n",
              "[16885] 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 1 1 0\n",
              "[16921] 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
              "[16957] 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0\n",
              "[16993] 0 1 1 0 0 0 0 0 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0\n",
              "[17029] 0 0 0 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0\n",
              "[17065] 1 1 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 0 1 0 0 1 0 0\n",
              "[17101] 1 0 0 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 0 1 0 0 1 1 0 0 1 1 1 0 0 0 0\n",
              "[17137] 1 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 1 0 1\n",
              "[17173] 0 1 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 0 0 1\n",
              "[17209] 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1\n",
              "[17245] 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0\n",
              "[17281] 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 1 0 0\n",
              "[17317] 1 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 1 0 0 1\n",
              "[17353] 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1\n",
              "[17389] 0 1 0 1 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1\n",
              "[17425] 1 0 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 0 1 0 1 0 1 1 0 0 1 1\n",
              "[17461] 1 1 0 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1\n",
              "[17497] 0 1 1 1 0 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 0 0 0 1 1\n",
              "[17533] 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 0 0 0 1 1\n",
              "[17569] 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 1 0 0 1 0 1 0 0 1 1 1 1 1 1 1 0 0 0 1\n",
              "[17605] 0 0 1 1 0 1 1 0 0 1 0 1 0 1 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 1 1\n",
              "[17641] 0 1 0 0 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 1 0 0 0 1 1 1 0 0 0 1 1 0 1\n",
              "[17677] 0 1 1 1\n",
              "Levels: 0 1"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Multinomial Naive Bayes classifier\n",
        "model2 <- naiveBayes(Labels ~ ., data = train_matrix, laplace = 1)\n",
        "\n",
        "# Predict on validation set\n",
        "valPred2 <- predict(model2, newdata = val_matrix[,-1])\n",
        "\n",
        "# Convert predictions and true labels to factors with the same levels\n",
        "all_levels <- c(0, 1)  # Set explicitly for binary classification\n",
        "valPred2 <- factor(valPred2, levels = all_levels)\n",
        "val_matrix$Labels <- factor(val_matrix$Labels, levels = all_levels)\n",
        "\n",
        "# Evaluate the model\n",
        "cm <- confusionMatrix(valPred2, val_matrix$Labels)\n",
        "print(cm)\n",
        "\n",
        "# Predict on test set\n",
        "#testPred2 <- predict(model2, newdata = test_matrix)\n",
        "\n",
        "# Ensure test predictions have the same factor levels (optional, depending on use case)\n",
        "#testPred2 <- factor(testPred2, levels = all_levels)\n",
        "\n",
        "# Output test predictions\n",
        "#print(head(testPred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUHGYvk8IkkS",
        "outputId": "8c94715c-38bf-47c6-b2f9-0ebfb3728b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix and Statistics\n",
            "\n",
            "          Reference\n",
            "Prediction    0    1\n",
            "         0 1126  127\n",
            "         1  412 1455\n",
            "                                          \n",
            "               Accuracy : 0.8272          \n",
            "                 95% CI : (0.8135, 0.8404)\n",
            "    No Information Rate : 0.5071          \n",
            "    P-Value [Acc > NIR] : < 2.2e-16       \n",
            "                                          \n",
            "                  Kappa : 0.6535          \n",
            "                                          \n",
            " Mcnemar's Test P-Value : < 2.2e-16       \n",
            "                                          \n",
            "            Sensitivity : 0.7321          \n",
            "            Specificity : 0.9197          \n",
            "         Pos Pred Value : 0.8986          \n",
            "         Neg Pred Value : 0.7793          \n",
            "             Prevalence : 0.4929          \n",
            "         Detection Rate : 0.3609          \n",
            "   Detection Prevalence : 0.4016          \n",
            "      Balanced Accuracy : 0.8259          \n",
            "                                          \n",
            "       'Positive' Class : 0               \n",
            "                                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Transformer**\n",
        "\n",
        "Alternative Approach: Transformers for Fake News Detection"
      ],
      "metadata": {
        "id": "XDmUXxg94-OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the Naive Bayes classifier is effective for many text classification tasks, modern approaches using transformer models have demonstrated superior performance. Transformers, such as BERT, utilize embeddings that capture contextual relationships in text, leading to better classification accuracy.\n",
        "\n",
        "In this section, we propose replacing the Naive Bayes classifier with a transformer-based model for the fake news detection task.\n"
      ],
      "metadata": {
        "id": "mc5Lxj7z5W9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this we will need to also install the packages:\n",
        "\n",
        "install.packages(\"torch\")\n",
        "install.packages(\"dplyr\")\n",
        "\n",
        "library(dplyr)\n",
        "torch::install_torch()\n",
        "# Install devtools if you don't have it\n",
        "install.packages(\"devtools\")\n",
        "\n",
        "# Use devtools to install the package from GitHub\n",
        "devtools::install_github(\"huggingface/transformers\")"
      ],
      "metadata": {
        "id": "glgt6M0v0Vsz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "04efb8eb-2e1b-4449-8e2e-95ac40c4fca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "\u001b[1m\u001b[33mError\u001b[39m in `check_supported_version()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[31m✖\u001b[39m Unsupported CUDA version \u001b[34m\"12.2\"\u001b[39m\n\u001b[36mℹ\u001b[39m Currently supported versions are: \u001b[34m\"11.7\"\u001b[39m and \u001b[34m\"11.8\"\u001b[39m.\n",
          "traceback": [
            "\u001b[1m\u001b[33mError\u001b[39m in `check_supported_version()`:\u001b[22m\n\u001b[1m\u001b[22m\u001b[31m✖\u001b[39m Unsupported CUDA version \u001b[34m\"12.2\"\u001b[39m\n\u001b[36mℹ\u001b[39m Currently supported versions are: \u001b[34m\"11.7\"\u001b[39m and \u001b[34m\"11.8\"\u001b[39m.\nTraceback:\n",
            "1. lantern_url()",
            "2. installation_kind()",
            "3. cuda_version()",
            "4. cuda_version_linux()",
            "5. check_supported_cuda_version_linux(cuda_version)",
            "6. check_supported_version(version, supported_versions)",
            "7. cli::cli_abort(c(x = \"Unsupported CUDA version {.val {version}}\", \n .     i = \"Currently supported versions are: {.val {supported_versions}}.\"))",
            "8. rlang::abort(message, ..., call = call, use_cli_format = TRUE, \n .     .frame = .frame)",
            "9. signal_abort(cnd, .file)",
            "10. signalCondition(cnd)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the datasets, the labels are converted to numeric,\n",
        "# and then we extract text from both training and test datasets"
      ],
      "metadata": {
        "id": "TgZ_UOcS5blf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the data with a pre-trained tokenizer\n",
        "# This way we convert texts to token IDs with embeddings\n",
        "\n",
        "tokenizer <- transformers::AutoTokenizer$from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "KPAY4aqM5c7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we create a dataset that holds tokenized input and corresponding labels"
      ],
      "metadata": {
        "id": "yQ7kbbrT5fHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After loading the pre-trained model,\n",
        "# we test it by running the data through it and get predictions."
      ],
      "metadata": {
        "id": "J2Dk6hco5gnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# then we evaluate the model by comparing the predictions to the actual labels,\n",
        "# and then calculate accuracy"
      ],
      "metadata": {
        "id": "sUK9Ll2i5hk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Findings\n",
        "\n",
        "1) We implemented the Naive Bayes Classifier first on a binary dataset (0,1), and then on the Fake News multi-class dataset (0,1,2,3,4,5). Even though the results of the first dataset were expected, the model on the multi-class dataset did not work accurately for the categorization of the news. We explain this by mentioning that:\n",
        "- the Naive Bayes Classifier Model assumes that features (e.g., words or phrases) are independent given the class label. In this particular example with the news articles, this assumption doesn’t hold. For example, certain phrases may often occur together in genuine articles but not in fake ones.\n",
        "- The model may not effectively capture the nuances that differentiate the two categories, especially if they share a lot of vocabulary.\n",
        "- It treats every feature independently and doesn’t consider the context or relationships between words.\n",
        "\n",
        "2) That lead us to our next step which was to use a model that takes into account the position of a token in a given phrase or sentence. The transformer is a good example of a model that uses Attention, adding embeddings to each token so as to capture semantic meanings, contextual relationships, and positional information.\n",
        "\n",
        "- By implementing a transformer-based model for fake news detection, we expect improved accuracy and reliability compared to the Naive Bayes classifier. The context-aware nature of transformers enables a deeper understanding of text, which is critical for accurately distinguishing between real and fake news.\n",
        "\n"
      ],
      "metadata": {
        "id": "p_P60KSR5liW"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}